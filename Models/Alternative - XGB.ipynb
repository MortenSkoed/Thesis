{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "2H9w-v24GMTW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EI8AOIdlwUf"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "Q9YVPE61GKxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smzlygdkmNBy"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "path1 = \"/content/drive/MyDrive/Thesis/Data/Merging CleanPrice & Features.csv\"\n",
        "price = pd.read_csv(path1, sep=',')\n",
        "\n",
        "price['Date'] = pd.to_datetime(price['Date'])\n",
        "columns = price.copy()\n",
        "price.set_index('Date', inplace=True)\n",
        "RawData = price.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframes and global settings"
      ],
      "metadata": {
        "id": "CuIgij_tGjbp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC9KgX_ifNGa"
      },
      "outputs": [],
      "source": [
        "# Placeholders\n",
        "placeholder = pd.DataFrame(index=pd.MultiIndex.from_product([columns['GEO_Name'].unique(), columns['Date'].unique()], names=['State', 'OriginalIndex']))\n",
        "horizons = [ 1, 3, 6, 12]\n",
        "hnames = ['h1', 'h3', 'h6', 'h12']\n",
        "\n",
        "for column_name in hnames:\n",
        "    placeholder[column_name] = np.nan\n",
        "\n",
        "# Placeholder df's\n",
        "Actuals = placeholder.copy()\n",
        "Forecasts = placeholder.copy()\n",
        "Forecast_errors = placeholder.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-DxaCWdwJF7"
      },
      "outputs": [],
      "source": [
        "# Global starting points\n",
        "Trials = 10\n",
        "Stopping = 25\n",
        "Random_Seed = 42\n",
        "jobs = 1\n",
        "\n",
        "# Initial settings\n",
        "validation_size = 24\n",
        "initial_train_size = 330\n",
        "retrain_period = 24"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "75ku2xzSGI_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WYRaIRHk85D"
      },
      "outputs": [],
      "source": [
        "def feature_selection(df, state, h):\n",
        "    State_Data = df[df['GEO_Name'] == state]\n",
        "    Log_return_lag = State_Data[[f'Log_Return_h{h}']].shift(h)\n",
        "    Log_return_lag = Log_return_lag.rename(lambda x: f'{x}_lag', axis='columns')\n",
        "    Log_return = State_Data[f'Log_Return_h{h}']\n",
        "    State_Data = State_Data.drop([f'Log_Return_h1', 'Log_Return_h3', 'Log_Return_h6', 'Log_Return_h12', 'Index_SA', 'Ln(Index_SA)'], axis=1).shift(h)\n",
        "    Lag = State_Data.drop(['Year', 'Month', 'GEO_Name'], axis=1).shift(h)\n",
        "    Lag = Lag.rename(lambda x: f'{x}_lag', axis='columns')\n",
        "    Combined = pd.concat([State_Data, Lag], axis=1).dropna()\n",
        "    Combined = pd.concat([Combined, Log_return_lag], axis=1).dropna()\n",
        "    Combined = pd.concat([Combined, Log_return], axis=1).dropna()\n",
        "\n",
        "    # Filter data\n",
        "    train = Combined.iloc[:initial_train_size-validation_size, :]\n",
        "    y = train[[f'Log_Return_h{h}']]\n",
        "    X = train.drop([f'Log_Return_h{h}', 'GEO_Name', 'Year', 'Month'], axis=1)\n",
        "\n",
        "    # Train decision tree\n",
        "    tree_model = DecisionTreeRegressor(random_state=Random_Seed)\n",
        "    tree_model.fit(X, y)\n",
        "\n",
        "    # Determine important features\n",
        "    feature_importances = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': tree_model.feature_importances_,\n",
        "        'GEO_Name': state,\n",
        "        'Count': 1,\n",
        "        'Horizon': f'h{h}'\n",
        "    }).sort_values(by='importance', ascending=False)\n",
        "    feature_importances['cumulative_importance'] = feature_importances['importance'].cumsum()\n",
        "    selected_features = feature_importances[feature_importances['cumulative_importance'] <= 0.95]['feature'].tolist()\n",
        "    selected_features_df = feature_importances[feature_importances['cumulative_importance'] <= 0.95]\n",
        "\n",
        "    print(selected_features)\n",
        "\n",
        "    return selected_features, Combined, selected_features_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_selection_loop(df, state, h, t):\n",
        "    State_Data = df[df['GEO_Name'] == state]\n",
        "    Log_return_lag = State_Data[[f'Log_Return_h{h}']].shift(h)\n",
        "    Log_return_lag = Log_return_lag.rename(lambda x: f'{x}_lag', axis='columns')\n",
        "    Log_return = State_Data[f'Log_Return_h{h}']\n",
        "    State_Data = State_Data.drop([f'Log_Return_h1', 'Log_Return_h3', 'Log_Return_h6', 'Log_Return_h12', 'Index_SA', 'Ln(Index_SA)'], axis=1).shift(h)\n",
        "    Lag = State_Data.drop(['Year', 'Month', 'GEO_Name'], axis=1).shift(h)\n",
        "    Lag = Lag.rename(lambda x: f'{x}_lag', axis='columns')\n",
        "    Combined = pd.concat([State_Data, Lag], axis=1).dropna()\n",
        "    Combined = pd.concat([Combined, Log_return_lag], axis=1).dropna()\n",
        "    Combined = pd.concat([Combined, Log_return], axis=1).dropna()\n",
        "\n",
        "    # Filter data\n",
        "    train = Combined.iloc[t - initial_train_size + retrain_period-1:t-validation_size + retrain_period-1, :]\n",
        "    y = train[[f'Log_Return_h{h}']]\n",
        "    X = train.drop([f'Log_Return_h{h}', 'GEO_Name', 'Year', 'Month'], axis=1)\n",
        "\n",
        "    # Train decision tree\n",
        "    tree_model = DecisionTreeRegressor(random_state=Random_Seed)\n",
        "    tree_model.fit(X, y)\n",
        "\n",
        "    # Determine important features\n",
        "    feature_importances = pd.DataFrame({\n",
        "        'feature': X.columns,\n",
        "        'importance': tree_model.feature_importances_,\n",
        "        'GEO_Name': state,\n",
        "        'Count': i,\n",
        "        'Horizon': f'h{h}'\n",
        "    }).sort_values(by='importance', ascending=False)\n",
        "    feature_importances['cumulative_importance'] = feature_importances['importance'].cumsum()\n",
        "    selected_features = feature_importances[feature_importances['cumulative_importance'] <= 0.95]['feature'].tolist()\n",
        "    selected_features_df = feature_importances[feature_importances['cumulative_importance'] <= 0.95]\n",
        "\n",
        "    print(selected_features)\n",
        "\n",
        "    return selected_features, Combined, selected_features_df"
      ],
      "metadata": {
        "id": "J2oAQGUPGQoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFJj063YwaTo"
      },
      "outputs": [],
      "source": [
        "def train_model_for_horizon(train, val, y_key, jobs=jobs):\n",
        "    Random_Seed1 = Random_Seed + i + w + q\n",
        "    def objective(trial):\n",
        "        # Hyperparameters to optimize\n",
        "        n_estimators = trial.suggest_int('n_estimators', 50, 500)\n",
        "        max_depth = trial.suggest_int('max_depth', 3, 7)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n",
        "        subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
        "        reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10, log=True)\n",
        "        reg_alpha = trial.suggest_float('reg_alpha', 0.01, 1, log=True)\n",
        "\n",
        "        model = xgb.XGBRegressor(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            learning_rate=learning_rate,\n",
        "            subsample=subsample,\n",
        "            reg_lambda=reg_lambda,\n",
        "            reg_alpha=reg_alpha,\n",
        "            n_jobs=jobs,\n",
        "            random_state= Random_Seed1,\n",
        "            tree_method = 'hist',\n",
        "            early_stopping_rounds = Stopping)\n",
        "\n",
        "        # Training the model\n",
        "        model.fit(\n",
        "            train.drop(columns=[y_key]), train[y_key],\n",
        "            eval_set=[(val.drop(columns=[y_key]), val[y_key])],\n",
        "            verbose=False)\n",
        "        preds = model.predict(val.drop(columns=[y_key]))\n",
        "\n",
        "        # Calculating MSE\n",
        "        mse = mean_squared_error(val[y_key], preds)\n",
        "\n",
        "        return -mse\n",
        "\n",
        "    # Optuna study for hyperparameter optimization\n",
        "    sampler = optuna.samplers.TPESampler(seed=Random_Seed1)\n",
        "    study = optuna.create_study(direction='maximize', sampler = sampler)\n",
        "    study.optimize(objective, n_trials=Trials, n_jobs=jobs)\n",
        "\n",
        "    # Best model training\n",
        "    best_params = study.best_params\n",
        "    best_model = xgb.XGBRegressor(\n",
        "        n_jobs=jobs,\n",
        "        random_state = Random_Seed1,\n",
        "        **best_params,\n",
        "        tree_method = 'hist',\n",
        "        early_stopping_rounds = Stopping)\n",
        "\n",
        "    # Combining train and validation sets for final model training\n",
        "    combined_train_val = pd.concat([train, val])\n",
        "    best_model.fit(\n",
        "        combined_train_val.drop(columns=[y_key]), combined_train_val[y_key],\n",
        "        eval_set=[(combined_train_val.drop(columns=[y_key]), combined_train_val[y_key])],\n",
        "        verbose=False)\n",
        "\n",
        "    feature_importances = pd.DataFrame({\n",
        "        'feature': combined_train_val.drop(columns=[y_key]).columns,\n",
        "        'importance': best_model.feature_importances_,\n",
        "        'GEO_Name': state,\n",
        "        'Count': i,\n",
        "        'Horizon': f'h{h}'\n",
        "    }).sort_values(by='importance', ascending=False)\n",
        "\n",
        "    return best_model, feature_importances"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "uQcTd4A_GR8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR4GUFc1m9O9"
      },
      "outputs": [],
      "source": [
        "# df's\n",
        "states = RawData['GEO_Name'].unique()\n",
        "all_selected_features_df = pd.DataFrame()\n",
        "Model_Importance = pd.DataFrame()\n",
        "\n",
        "w = 0\n",
        "q = 0\n",
        "\n",
        "for state in states:\n",
        "    StateData = RawData[RawData['GEO_Name'] == state]\n",
        "\n",
        "    q += 1\n",
        "\n",
        "    for h, hname in zip(horizons, hnames):\n",
        "\n",
        "        w += 1\n",
        "\n",
        "        print(f\"\\nProcessing: {state}\")\n",
        "        print(f'Horizon: {h} \\n')\n",
        "\n",
        "        selected_feature_list, Combined, selected_features_df = feature_selection(RawData, state, h)\n",
        "        all_selected_features_df = pd.concat([all_selected_features_df, selected_features_df], ignore_index=True)\n",
        "\n",
        "        # Defining Target & Feature\n",
        "        Target = Combined[[f'Log_Return_h{h}']]\n",
        "        Feature = Combined[selected_feature_list]\n",
        "\n",
        "        Data = pd.concat([Target, Feature], axis=1).dropna()\n",
        "\n",
        "        y_key = f'Log_Return_h{h}'\n",
        "\n",
        "        # Initial traning and validation\n",
        "        train_initial = Data.iloc[:initial_train_size-validation_size, :]\n",
        "        val_initial = Data.iloc[initial_train_size-validation_size:initial_train_size, :]\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        model, feature_importances = train_model_for_horizon(train_initial, val_initial, y_key, jobs=jobs)\n",
        "\n",
        "        Model_Importance = pd.concat([Model_Importance, feature_importances], ignore_index=True)\n",
        "\n",
        "        test = Data.iloc[initial_train_size+h:initial_train_size+retrain_period+h, :]\n",
        "        forecast = model.predict(test.drop(columns=[y_key]))\n",
        "\n",
        "        for i in range(retrain_period):\n",
        "          Forecasts.loc[(state, test.index[i]), hname] = forecast[i]\n",
        "          Actuals.loc[(state, test.index[i]), hname] = test[y_key].values[i]\n",
        "\n",
        "        for t in range(initial_train_size, len(Data), 1):\n",
        "\n",
        "            # Cross validation (Nested Validation)\n",
        "            if i % retrain_period == 0:\n",
        "\n",
        "                selected_feature_list, Combined, selected_features_df = feature_selection_loop(RawData, state, h, t)\n",
        "                all_selected_features_df = pd.concat([all_selected_features_df, selected_features_df], ignore_index=True)\n",
        "\n",
        "                # Defining Target & Feature\n",
        "                Target = Combined[[f'Log_Return_h{h}']]\n",
        "                Feature = Combined[selected_feature_list]\n",
        "                Data = pd.concat([Target, Feature], axis=1).dropna()\n",
        "                train = Data.iloc[t - initial_train_size + retrain_period-1:t - validation_size + retrain_period-1, :]\n",
        "                val = Data.iloc[t-validation_size + retrain_period-1:t + retrain_period-1, :]\n",
        "\n",
        "                model, feature_importances = train_model_for_horizon(train, val, y_key, jobs=1)\n",
        "\n",
        "                Model_Importance = pd.concat([Model_Importance, feature_importances], ignore_index=True)\n",
        "\n",
        "                print(f'\\n Length {t}')\n",
        "\n",
        "            i += 1\n",
        "\n",
        "            test = Data.iloc[t+retrain_period+h-1:t+retrain_period+h, :]\n",
        "            if test.empty:\n",
        "              break\n",
        "\n",
        "            forecast = model.predict(test.drop(columns=[y_key]))\n",
        "\n",
        "            # Appending predictions\n",
        "            Forecasts.loc[(state, test.index[0]), hname] = forecast[0]\n",
        "            Actuals.loc[(state, test.index[0]), hname] = test[y_key].values[0]\n",
        "\n",
        "Forecasts = Forecasts.reset_index(level='State')\n",
        "Actuals = Actuals.reset_index(level='State')\n",
        "\n",
        "# Feature Selection\n",
        "importance_summary = all_selected_features_df.groupby(['feature', 'GEO_Name', 'Horizon'])['importance'].sum().reset_index()\n",
        "count_summary = all_selected_features_df.groupby(['GEO_Name', 'Horizon'])['Count'].nunique().reset_index()\n",
        "count_summary.rename(columns={'Count': 'Unique_Counts'}, inplace=True)\n",
        "summary = importance_summary.merge(count_summary, on=['GEO_Name', 'Horizon'], how='left')\n",
        "summary['mean_importance'] = summary['importance'] / summary['Unique_Counts']\n",
        "summary.drop(columns=['Unique_Counts', 'importance'], inplace=True)\n",
        "summary_sorted = summary.sort_values(by=['GEO_Name', 'Horizon', 'mean_importance'], ascending=[True, True, False]).reset_index()\n",
        "summary_sorted.drop(columns=['index'], inplace=True)\n",
        "\n",
        "# Feature Importance\n",
        "Importance_Model = Model_Importance.groupby(['feature', 'GEO_Name', 'Horizon'])['importance'].sum().reset_index()\n",
        "count_summary_model = Model_Importance.groupby(['GEO_Name', 'Horizon'])['Count'].nunique().reset_index()\n",
        "count_summary_model.rename(columns={'Count': 'Unique_Counts'}, inplace=True)\n",
        "summary_model = Importance_Model.merge(count_summary_model, on=['GEO_Name', 'Horizon'], how='left')\n",
        "summary_model['mean_importance'] = summary_model['importance'] / summary_model['Unique_Counts']\n",
        "summary_model.drop(columns=['Unique_Counts', 'importance'], inplace=True)\n",
        "summary_model_sorted = summary_model.sort_values(by=['GEO_Name', 'Horizon', 'mean_importance'], ascending=[True, True, False]).reset_index()\n",
        "summary_model_sorted.drop(columns=['index'], inplace=True)\n",
        "\n",
        "# Saving CSV of Predictions, Actuals, Feature Selection and Importances\n",
        "Forecasts.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/XGBoostPredictions.csv\")\n",
        "Actuals.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/XGBoostActuals.csv\")\n",
        "all_selected_features_df.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/XGBoostFeatureImportance.csv\")\n",
        "Model_Importance.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/XGBoostFeatureImportanceModel.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "Npdwwd6ABfr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}