{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "LP9bzKQuYodk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74tTklCe59cU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "!pip install pmdarima\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "wMNnKZ0xY-zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "path = \"/content/drive/MyDrive/Thesis/Data/Merging CleanPrice & Features.csv\"\n",
        "price = pd.read_csv(path, sep=',')\n",
        "\n",
        "price['Date'] = pd.to_datetime(price['Date'])"
      ],
      "metadata": {
        "id": "qRJdFYES6QOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframes and global settings"
      ],
      "metadata": {
        "id": "bEe7H7_VZm1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract dataframe that only has our y, state name, and date\n",
        "log_return = price[['Date', 'GEO_Name', 'Log_Return_h1']].dropna()\n",
        "\n",
        "# Set index to the Date\n",
        "log_return['Date'] = pd.to_datetime(log_return['Date'])\n",
        "log_return.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "id": "8JNqnNIsViwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The initial train size and max forecast length\n",
        "initial_train_size = 330\n",
        "max_forecast = 12"
      ],
      "metadata": {
        "id": "aYA28ItyVrU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function"
      ],
      "metadata": {
        "id": "m3ILgUwBZaCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fit the ARIMA\n",
        "def fit_arima_model(data, arima_order):\n",
        "    model = ARIMA(data, order=arima_order, freq='MS')\n",
        "    return model.fit()"
      ],
      "metadata": {
        "id": "Kd83MILMVs6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions"
      ],
      "metadata": {
        "id": "xzPKkgKDZfP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame to save the forecasts and actual values\n",
        "all_forecasts = pd.DataFrame()\n",
        "all_actuals = pd.DataFrame()\n",
        "\n",
        "# DataFrame to log ARIMA orders\n",
        "order_log = pd.DataFrame(columns=['State', 'Roll', 'p', 'd', 'q', 'AIC'])\n",
        "\n",
        "# Loop through each unique state\n",
        "for state in log_return['GEO_Name'].unique():\n",
        "    print(f\"Processing state: {state}\")\n",
        "    print(\"\\n\")\n",
        "    state_data = log_return[log_return['GEO_Name'] == state]\n",
        "    state_data = state_data.asfreq('MS')\n",
        "\n",
        "    total_obs = len(state_data)\n",
        "    forecast_matrix = []\n",
        "    actual_matrix = []\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    # Inner loop for forecasting per state\n",
        "    while initial_train_size + i < total_obs:\n",
        "        training_data = state_data.iloc[i:initial_train_size + i]\n",
        "\n",
        "        # Recursive update of the order\n",
        "        if i % 24 == 0:\n",
        "            try:\n",
        "                auto_model = auto_arima(training_data['Log_Return_h1'], start_p=1, start_q=0,\n",
        "                                        max_p=12, max_q=0, d=0, seasonal=False,\n",
        "                                        trace=False, error_action='ignore', suppress_warnings=True,\n",
        "                                        stepwise=True, information_criterion='aic',\n",
        "                                        n_jobs=1)\n",
        "                order = auto_model.order\n",
        "                p, d, q = order\n",
        "                aic_score = auto_model.aic()\n",
        "                print(f\"Updated ARIMA order to (p={p}, d={d}, q={q}) with AIC {aic_score} at index {initial_train_size + i}\")\n",
        "                # Log the order and AIC score\n",
        "                new_row = pd.DataFrame({\n",
        "                    'State': [state],\n",
        "                    'Roll': [initial_train_size + i],\n",
        "                    'p': [p],\n",
        "                    'd': [d],\n",
        "                    'q': [q],\n",
        "                    'AIC': [aic_score]\n",
        "                })\n",
        "                order_log = pd.concat([order_log, new_row], ignore_index=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error updating ARIMA order at index {initial_train_size + i}: {e}\")\n",
        "\n",
        "        # Fit ARIMA model with the current order\n",
        "        arima_model = fit_arima_model(training_data['Log_Return_h1'], order)\n",
        "\n",
        "        # Determine forecast range dynamically\n",
        "        forecast_steps = min(max_forecast, total_obs - (initial_train_size + i))\n",
        "\n",
        "        # Forecast for the range\n",
        "        forecast_result = arima_model.forecast(steps=forecast_steps)\n",
        "        forecast_matrix.append(forecast_result.values)\n",
        "\n",
        "        # Collect actual values\n",
        "        actual_values = state_data['Log_Return_h1'].iloc[initial_train_size + i: initial_train_size + i + forecast_steps]\n",
        "        actual_matrix.append(actual_values.values)\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    # Collect forecasts and actuals for the current state\n",
        "    forecast_df = pd.DataFrame(forecast_matrix, index=pd.RangeIndex(start=initial_train_size, stop=initial_train_size+len(forecast_matrix)))\n",
        "    actual_df = pd.DataFrame(actual_matrix, index=pd.RangeIndex(start=initial_train_size, stop=initial_train_size+len(actual_matrix)))\n",
        "\n",
        "    forecast_df['state'] = state\n",
        "    actual_df['state'] = state\n",
        "\n",
        "    all_forecasts = pd.concat([all_forecasts, forecast_df], ignore_index=False)\n",
        "    all_actuals = pd.concat([all_actuals, actual_df], ignore_index=False)\n"
      ],
      "metadata": {
        "id": "JDjDKBKtk-Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order_log.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)_ORDER.csv\")\n",
        "order_log.to_excel(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)_ORDER.xlsx\")"
      ],
      "metadata": {
        "id": "TlV3_scPLVLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatting dataframes and saving predictions and actual values"
      ],
      "metadata": {
        "id": "ItU2lvu6Z9l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's change the column names and the index\n",
        "\n",
        "# We want the index to start at the train value, and just call it 'origin'\n",
        "all_forecasts.index.name = 'Origin'\n",
        "\n",
        "# Define a function to rename columns\n",
        "def rename_columns(df):\n",
        "    renamed_columns = []\n",
        "    for col in df.columns:\n",
        "        if col != 'state':\n",
        "            if isinstance(col, int) and 0 <= col <= 11:\n",
        "                renamed_columns.append('h' + str(col + 1))\n",
        "            else:\n",
        "                renamed_columns.append(col)\n",
        "        else:\n",
        "            renamed_columns.append(col)\n",
        "    return renamed_columns\n",
        "\n",
        "# Rename columns of all_forecasts and actuals DataFrame\n",
        "all_forecasts.columns = rename_columns(all_forecasts)\n",
        "all_actuals.columns = rename_columns(all_actuals)"
      ],
      "metadata": {
        "id": "MZGNzE1enST7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_forecasts.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)PredictionsFormat.csv\")\n",
        "all_actuals.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)ActualsFormat.csv\")\n",
        "\n",
        "path2 = \"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)ActualsFormat.csv\"\n",
        "path3 = \"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)PredictionsFormat.csv\""
      ],
      "metadata": {
        "id": "_961l3gXOSUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading agian\n",
        "Actuals_load = pd.read_csv(path2, sep=',')\n",
        "Predictions_load = pd.read_csv(path3, sep=',')\n",
        "\n",
        "columns = price.copy()\n",
        "Actuals_load.rename(columns={'Unnamed: 0': 'Origin'}, inplace=True)"
      ],
      "metadata": {
        "id": "c2mCzklAhq1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summing in these different dataframes and creating a new column that has the prediction for the horizon"
      ],
      "metadata": {
        "id": "MYSG-VuGar_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes that we can get the different horizons from\n",
        "forecast_h1 = Predictions_load[['state', 'h1', 'Origin']].dropna()\n",
        "forecast_h3 = Predictions_load[['state', 'h1', 'h2', 'h3', 'Origin']].dropna()\n",
        "forecast_h6 = Predictions_load[['state', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'Origin']].dropna()\n",
        "forecast_h12 = Predictions_load.copy().dropna()"
      ],
      "metadata": {
        "id": "cFdiN_Xdar_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# h3\n",
        "h3_sum = ['h1', 'h2', 'h3']\n",
        "forecast_h3['Prediction_h3'] = forecast_h3[h3_sum].sum(axis=1)\n",
        "columns_to_drop = ['h1', 'h2', 'h3']\n",
        "forecast_h3 = forecast_h3.drop(columns=columns_to_drop)\n",
        "\n",
        "# h6\n",
        "h6_sum = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
        "forecast_h6['Prediction_h6'] = forecast_h6[h6_sum].sum(axis=1)\n",
        "columns_to_drop = ['h1', 'h2', 'h3','h4', 'h5', 'h6']\n",
        "forecast_h6 = forecast_h6.drop(columns=columns_to_drop)\n",
        "\n",
        "# h12\n",
        "h12_sum = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'h9', 'h10', 'h11', 'h12']\n",
        "forecast_h12['Prediction_h12'] = forecast_h12[h12_sum].sum(axis=1)\n",
        "columns_to_drop = ['h1', 'h2', 'h3','h4', 'h5', 'h6', 'h7', 'h8', 'h9', 'h10', 'h11', 'h12']\n",
        "forecast_h12 = forecast_h12.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "r1aHY14-ar_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same for actuals"
      ],
      "metadata": {
        "id": "dxObCg7va4gB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframes that we can get the different horizons from\n",
        "actuals_h1 = Actuals_load[['state', 'h1', 'Origin']].dropna()\n",
        "actuals_h3 = Actuals_load[['state', 'h1', 'h2', 'h3', 'Origin']].dropna()\n",
        "actuals_h6 = Actuals_load[['state', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'Origin']].dropna()\n",
        "actuals_h12 = Actuals_load.copy().dropna()"
      ],
      "metadata": {
        "id": "xnKmQ9oua4gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# h3\n",
        "h3_sum = ['h1', 'h2', 'h3']\n",
        "actuals_h3['actuals_h3'] = actuals_h3[h3_sum].sum(axis=1)\n",
        "columns_to_drop = ['h1', 'h2', 'h3']\n",
        "actuals_h3 = actuals_h3.drop(columns=columns_to_drop)\n",
        "\n",
        "# h6\n",
        "h6_sum = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']\n",
        "actuals_h6['actuals_h6'] = actuals_h6[h6_sum].sum(axis=1)\n",
        "columns_to_drop = ['h1', 'h2', 'h3','h4', 'h5', 'h6']\n",
        "actuals_h6 = actuals_h6.drop(columns=columns_to_drop)\n",
        "\n",
        "# h12\n",
        "h12_sum = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'h8', 'h9', 'h10', 'h11', 'h12']\n",
        "actuals_h12['actuals_h12'] = actuals_h12[h12_sum].sum(axis=1)\n",
        "columns_to_drop = ['h1', 'h2', 'h3','h4', 'h5', 'h6', 'h7', 'h8', 'h9', 'h10', 'h11', 'h12']\n",
        "actuals_h12 = actuals_h12.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "pDHFoorXa4gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Dataframes with Actuals and Predictions"
      ],
      "metadata": {
        "id": "0drFZFEebabQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# h1\n",
        "# Extract dataframe that only has our y, state name, and date\n",
        "origins2 = price[['Date', 'GEO_Name']].dropna()\n",
        "origins = origins2[origins2['GEO_Name'] == 'AK']\n",
        "origins['PredictionDate'] = columns['Date']\n",
        "origins_h1 = origins[['PredictionDate']].shift(-1).dropna().reset_index()\n",
        "origins_h1.rename(columns={'index': 'Origin'}, inplace=True)\n",
        "\n",
        "# h3\n",
        "origins['PredictionDate'] = columns['Date']\n",
        "origins_h3 = origins[['PredictionDate']].shift(-3).dropna().reset_index()\n",
        "origins_h3.rename(columns={'index': 'Origin'}, inplace=True)\n",
        "\n",
        "# h6\n",
        "origins['PredictionDate'] = columns['Date']\n",
        "origins_h6 = origins[['PredictionDate']].shift(-6).dropna().reset_index()\n",
        "origins_h6.rename(columns={'index': 'Origin'}, inplace=True)\n",
        "\n",
        "# h12\n",
        "origins['PredictionDate'] = columns['Date']\n",
        "origins_h12 = origins[['PredictionDate']].shift(-12).dropna().reset_index()\n",
        "origins_h12.rename(columns={'index': 'Origin'}, inplace=True)"
      ],
      "metadata": {
        "id": "7WiFGuS5babR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Forecasts ##\n",
        "# h1\n",
        "prediction_h1 = pd.merge(forecast_h1, origins_h1, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "prediction_h1.drop(columns=['index', 'Origin'], inplace=True)\n",
        "\n",
        "# h3\n",
        "prediction_h3 = pd.merge(forecast_h3, origins_h3, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "prediction_h3.drop(columns=['index', 'Origin'], inplace=True)\n",
        "\n",
        "# h6\n",
        "prediction_h6 = pd.merge(forecast_h6, origins_h6, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "prediction_h6.drop(columns=['index', 'Origin'], inplace=True)\n",
        "\n",
        "# h12\n",
        "prediction_h12 = pd.merge(forecast_h12, origins_h12, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "prediction_h12.drop(columns=['index', 'Origin'], inplace=True)"
      ],
      "metadata": {
        "id": "El2Hk-JAbabU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Actuals ##\n",
        "# h1\n",
        "Returns_h1 = pd.merge(actuals_h1, origins_h1, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "Returns_h1.drop(columns=['index', 'Origin'], inplace=True)\n",
        "\n",
        "# h3\n",
        "Returns_h3 = pd.merge(actuals_h3, origins_h3, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "Returns_h3.drop(columns=['index', 'Origin'], inplace=True)\n",
        "\n",
        "# h6\n",
        "Returns_h6 = pd.merge(actuals_h6, origins_h6, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "Returns_h6.drop(columns=['index', 'Origin'], inplace=True)\n",
        "\n",
        "# h12\n",
        "Returns_h12 = pd.merge(actuals_h12, origins_h12, on='Origin', how='inner').sort_values(by=['state', 'Origin']).reset_index()\n",
        "Returns_h12.drop(columns=['index', 'Origin'], inplace=True)"
      ],
      "metadata": {
        "id": "lZ6gV_FcbabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting indexes\n",
        "Returns_h1.set_index(['state', 'PredictionDate'], inplace=True)\n",
        "Returns_h3.set_index(['state', 'PredictionDate'], inplace=True)\n",
        "Returns_h6.set_index(['state', 'PredictionDate'], inplace=True)\n",
        "Returns_h12.set_index(['state', 'PredictionDate'], inplace=True)\n",
        "\n",
        "prediction_h1.set_index(['state', 'PredictionDate'], inplace=True)\n",
        "prediction_h3.set_index(['state', 'PredictionDate'], inplace=True)\n",
        "prediction_h6.set_index(['state', 'PredictionDate'], inplace=True)\n",
        "prediction_h12.set_index(['state', 'PredictionDate'], inplace=True)"
      ],
      "metadata": {
        "id": "FnOrIwirbabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining the DataFrames\n",
        "Actuals = Returns_h1.join([Returns_h3, Returns_h6, Returns_h12], how='outer')\n",
        "Forecasts = prediction_h1.join([prediction_h3, prediction_h6, prediction_h12], how='outer')"
      ],
      "metadata": {
        "id": "ajITWLl3babV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming columns\n",
        "Forecasts.rename(columns={'Prediction_h3': 'h3',\n",
        "                             'Prediction_h6': 'h6',\n",
        "                             'Prediction_h12': 'h12'}, inplace=True)\n",
        "\n",
        "Actuals.rename(columns={'actuals_h3': 'h3',\n",
        "                             'actuals_h6': 'h6',\n",
        "                             'actuals_h12': 'h12'}, inplace=True)"
      ],
      "metadata": {
        "id": "suOvbQ9sbabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resetting index\n",
        "Forecasts = Forecasts.reset_index(level='state')\n",
        "Actuals = Actuals.reset_index(level='state')"
      ],
      "metadata": {
        "id": "5iIE0xRhbabW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to csv\n",
        "Forecasts.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)Predictions.csv\")\n",
        "Actuals.to_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)Actuals.csv\")"
      ],
      "metadata": {
        "id": "UPyyD1BhbabW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "odmyWCWgborD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}