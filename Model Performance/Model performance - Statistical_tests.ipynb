{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "eyevaWOHXMBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "g-Qjydrrzs0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and processing Data"
      ],
      "metadata": {
        "id": "yoHqKQsW7cGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading predictions and corresponding actuals\n",
        "actuals = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(1)Actuals.csv\", sep=',')\n",
        "AR_1 = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(1)Predictions.csv\", sep=',')\n",
        "AR_optimal = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)Predictions.csv\", sep=',')\n",
        "ARIMA = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/ARIMAPredictions.csv\", sep=',')\n",
        "RandomForest = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/RandomForestPredictions.csv\", sep=',')\n",
        "XGBoost = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/XGBoostPredictions.csv\", sep=',')"
      ],
      "metadata": {
        "id": "iH2JBhnDz9Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the date column to date\n",
        "actuals.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "AR_1.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "AR_optimal.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "ARIMA.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "\n",
        "RandomForest.rename(columns={'OriginalIndex': 'Date'}, inplace=True)\n",
        "XGBoost.rename(columns={'OriginalIndex': 'Date'}, inplace=True)\n",
        "\n",
        "RandomForest.rename(columns={'State': 'state'}, inplace=True)\n",
        "XGBoost.rename(columns={'State': 'state'}, inplace=True)"
      ],
      "metadata": {
        "id": "PHkI9bQWMkwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Combination forecasts\n",
        "combined_df1 = pd.concat([AR_optimal, ARIMA, RandomForest, XGBoost], ignore_index=True)\n",
        "averaged_predictions1 = combined_df1.groupby(['Date', 'state']).mean().reset_index()\n",
        "Combination_AR_1 = averaged_predictions1.sort_values(by=['state', 'Date']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "E5u-7rCeErJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'USA' from the predictions and actuals\n",
        "actuals = actuals[actuals['state'] != 'USA']\n",
        "AR_1 = AR_1[AR_1['state'] != 'USA']\n",
        "AR_optimal = AR_optimal[AR_optimal['state'] != 'USA']\n",
        "ARIMA = ARIMA[ARIMA['state'] != 'USA']\n",
        "RandomForest = RandomForest[RandomForest['state'] != 'USA']\n",
        "XGBoost = XGBoost[XGBoost['state'] != 'USA']\n",
        "Combination_AR_1 = Combination_AR_1[Combination_AR_1['state'] != 'USA']"
      ],
      "metadata": {
        "id": "qvGt4XTv-El8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the dataframes to datetime\n",
        "actuals['Date'] = pd.to_datetime(actuals['Date'])\n",
        "AR_1['Date'] = pd.to_datetime(AR_1['Date'])\n",
        "AR_optimal['Date'] = pd.to_datetime(AR_optimal['Date'])\n",
        "ARIMA['Date'] = pd.to_datetime(ARIMA['Date'])\n",
        "RandomForest['Date'] = pd.to_datetime(RandomForest['Date'])\n",
        "XGBoost['Date'] = pd.to_datetime(XGBoost['Date'])\n",
        "Combination_AR_1['Date'] = pd.to_datetime(Combination_AR_1['Date'])"
      ],
      "metadata": {
        "id": "HvuuwhBEKl0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing the DM test for entire out-of-sample period"
      ],
      "metadata": {
        "id": "8b_Qrip87qbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the dataframes to have the same length\n",
        "actuals = actuals[actuals['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "AR_1 = AR_1[AR_1['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "AR_optimal = AR_optimal[AR_optimal['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "ARIMA = ARIMA[ARIMA['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "RandomForest = RandomForest[RandomForest['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "XGBoost = XGBoost[XGBoost['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "Combination_AR_1 = Combination_AR_1[Combination_AR_1['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "wI7Mw7pjCHOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of actuals and models used in performing the DM test\n",
        "actuals1 = actuals\n",
        "models = {\n",
        "    'AR_1': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "m-Zavyt88Z85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "states = actuals['state'].unique()\n",
        "\n",
        "# Handling data on state and horizon key\n",
        "errors = {state: {h: {} for h in horizons} for state in states}\n",
        "\n",
        "# Calculating errors based on actuals and the respective model\n",
        "for state in states:\n",
        "    for model_name, df_model in models.items():\n",
        "        for h in horizons:\n",
        "            actuals_state = actuals1[actuals1['state'] == state]\n",
        "            df_merged = pd.merge(actuals_state[['state', 'Date', h]], df_model[['state', 'Date', h]], on=['state', 'Date'], how='inner', suffixes=('_act', '_pred'))\n",
        "            errors[state][h][model_name] = df_merged[f'{h}_act'] - df_merged[f'{h}_pred']"
      ],
      "metadata": {
        "id": "-y2A-3at9TJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buYCWs4OyxOb"
      },
      "outputs": [],
      "source": [
        "# Diebold-Mariano test\n",
        "\n",
        "def Diebold_Mariano_Test(model_errors,benchmark_erros, h):\n",
        "    LossFunc = np.array(benchmark_erros.dropna())**2 - np.array(model_errors.dropna())**2\n",
        "    maxLag = h\n",
        "    DM = pd.DataFrame({'y':LossFunc})\n",
        "    reg = smf.ols('y~1', data=DM).fit(cov_type='HAC',cov_kwds={'maxlags':np.max(maxLag),'kernel':'bartlett'})\n",
        "    intercept = reg.params['Intercept']\n",
        "    test_statistic = reg.tvalues['Intercept']\n",
        "    p_Value = 1-norm.cdf(test_statistic)\n",
        "\n",
        "    return p_Value, intercept, test_statistic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = actuals['state'].unique()\n",
        "horizons = [1, 3, 6, 12]\n",
        "Benchmark = 'AR_1'\n",
        "\n",
        "# Placeholder for DM test results\n",
        "DM_results = pd.DataFrame(columns=['State', 'Horizon', 'Model', 'P_Value', 'DM_Statistic', 'Params'])\n",
        "\n",
        "# Performing the Diebold-Mariano test excluding the benchmark in the results dataframe\n",
        "for state in states:\n",
        "    for h in horizons:\n",
        "        for model_name in models.keys():\n",
        "            if model_name != Benchmark:\n",
        "                p_Value, intercept, test_statistic = Diebold_Mariano_Test(errors[state][f'h{h}'][model_name], errors[state][f'h{h}'][Benchmark], h=h)\n",
        "                # Creating a new df with results\n",
        "                new_row = pd.DataFrame({\n",
        "                    'State': [state],\n",
        "                    'Horizon': [f'h{h}'],\n",
        "                    'Model': [model_name],\n",
        "                    'P_Value': [p_Value],\n",
        "                    'DM_Statistic': [test_statistic],\n",
        "                    'Params': [intercept]\n",
        "                })\n",
        "                # Concat results into one df\n",
        "                DM_results = pd.concat([DM_results, new_row], ignore_index=True)"
      ],
      "metadata": {
        "id": "4kZnIest-Wre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DM_results.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/DieboldMarianoTestResults.xlsx\")"
      ],
      "metadata": {
        "id": "NSXWcBvBLpwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performing the DM test for out-of-sample period excluding crisis period"
      ],
      "metadata": {
        "id": "b4wJY_GRCUy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluding crisis\n",
        "actuals = actuals[(actuals['Date'] >= pd.Timestamp('2011-01-01')) & (actuals['Date'] <= pd.Timestamp('2019-12-01'))].reset_index(drop=True)\n",
        "AR_1 = AR_1[(AR_1['Date'] >= pd.Timestamp('2011-01-01')) & (AR_1['Date'] <= pd.Timestamp('2019-12-01'))].reset_index(drop=True)\n",
        "AR_optimal = AR_optimal[(AR_optimal['Date'] >= pd.Timestamp('2011-01-01')) & (AR_optimal['Date'] <= pd.Timestamp('2019-12-01'))].reset_index(drop=True)\n",
        "ARIMA = ARIMA[(ARIMA['Date'] >= pd.Timestamp('2011-01-01')) & (ARIMA['Date'] <= pd.Timestamp('2019-12-01'))].reset_index(drop=True)\n",
        "XGBoost = XGBoost[(XGBoost['Date'] >= pd.Timestamp('2011-01-01')) & (XGBoost['Date'] <= pd.Timestamp('2019-12-01'))].reset_index(drop=True)\n",
        "RandomForest = RandomForest[(RandomForest['Date'] >= pd.Timestamp('2011-01-01')) & (RandomForest['Date'] <= pd.Timestamp('2019-12-01'))].reset_index(drop=True)\n",
        "Combination_AR_1 = Combination_AR_1[(Combination_AR_1['Date'] >= pd.Timestamp('2011-01-01')) & (Combination_AR_1['Date'] <= pd.Timestamp('2019-12-01'))].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "sym4lyvNCbKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of actuals and models used in performing the DM test\n",
        "actuals1 = actuals\n",
        "models = {\n",
        "    'AR_1': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "twRIafBZCbKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "states = actuals['state'].unique()\n",
        "\n",
        "# Handling data on state and horizon key\n",
        "errors = {state: {h: {} for h in horizons} for state in states}\n",
        "\n",
        "# Calculating errors based on actuals and the respective model\n",
        "for state in states:\n",
        "    for model_name, df_model in models.items():\n",
        "        for h in horizons:\n",
        "            actuals_state = actuals1[actuals1['state'] == state]\n",
        "            df_merged = pd.merge(actuals_state[['state', 'Date', h]], df_model[['state', 'Date', h]], on=['state', 'Date'], how='inner', suffixes=('_act', '_pred'))\n",
        "            errors[state][h][model_name] = df_merged[f'{h}_act'] - df_merged[f'{h}_pred']"
      ],
      "metadata": {
        "id": "JQqHBTUwCbKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXHmoGGrCbKq"
      },
      "outputs": [],
      "source": [
        "# Diebold-Mariano test\n",
        "\n",
        "def Diebold_Mariano_Test(model_errors,benchmark_erros, h):\n",
        "    LossFunc = np.array(benchmark_erros.dropna())**2 - np.array(model_errors.dropna())**2\n",
        "    maxLag = h\n",
        "    DM = pd.DataFrame({'y':LossFunc})\n",
        "    reg = smf.ols('y~1', data=DM).fit(cov_type='HAC',cov_kwds={'maxlags':np.max(maxLag),'kernel':'bartlett'})\n",
        "    intercept = reg.params['Intercept']\n",
        "    test_statistic = reg.tvalues['Intercept']\n",
        "    p_Value = 1-norm.cdf(test_statistic)\n",
        "\n",
        "    return p_Value, intercept, test_statistic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = actuals['state'].unique()\n",
        "horizons = [1, 3, 6, 12]\n",
        "Benchmark = 'AR_1'\n",
        "\n",
        "# Placeholder for DM test results\n",
        "DM_results_excl_crisis = pd.DataFrame(columns=['State', 'Horizon', 'Model', 'P_Value', 'DM_Statistic', 'Params'])\n",
        "\n",
        "# Performing the Diebold-Mariano test excluding the benchmark in the results dataframe\n",
        "for state in states:\n",
        "    for h in horizons:\n",
        "        for model_name in models.keys():\n",
        "            if model_name != Benchmark:\n",
        "                p_Value, intercept, test_statistic = Diebold_Mariano_Test(errors[state][f'h{h}'][model_name], errors[state][f'h{h}'][Benchmark], h=h)\n",
        "                # Creating a new df with results\n",
        "                new_row = pd.DataFrame({\n",
        "                    'State': [state],\n",
        "                    'Horizon': [f'h{h}'],\n",
        "                    'Model': [model_name],\n",
        "                    'P_Value': [p_Value],\n",
        "                    'DM_Statistic': [test_statistic],\n",
        "                    'Params': [intercept]\n",
        "                })\n",
        "                # Concat results into one df\n",
        "                DM_results_excl_crisis = pd.concat([DM_results_excl_crisis, new_row], ignore_index=True)"
      ],
      "metadata": {
        "id": "q5ZTqkC2CbKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DM_results_excl_crisis.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/ExclCrisisDieboldMarianoTestResults.xlsx\")"
      ],
      "metadata": {
        "id": "Fv-NzTtmYgmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "3Cj1woTbYRSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}