{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "WBAoyhMyVHiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoEL7L2V6O94"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actuals for mean benchmark"
      ],
      "metadata": {
        "id": "BFAW_5Wm28Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "path1 = \"/content/drive/MyDrive/Thesis/Data/Merging CleanPrice & Features.csv\"\n",
        "price = pd.read_csv(path1, sep=',')\n",
        "\n",
        "columns = price.copy()\n",
        "price.set_index('Date', inplace=True)\n",
        "RawData = price.copy()"
      ],
      "metadata": {
        "id": "aNHvjiqi2-PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Mean Benchmark"
      ],
      "metadata": {
        "id": "VL7Wm9J47JQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making rolling mean benchmark\n",
        "columns = ['GEO_Name', 'Log_Return_h1', 'Log_Return_h3', 'Log_Return_h6', 'Log_Return_h12']\n",
        "actual_data = RawData[columns].copy()\n",
        "mean_dfs = []\n",
        "\n",
        "for name, group in actual_data.groupby('GEO_Name'):\n",
        "    group['Mean_h1'] = group['Log_Return_h1'].rolling(window=330).mean().shift(1)\n",
        "    group['Mean_h3'] = group['Log_Return_h3'].rolling(window=330).mean().shift(3)\n",
        "    group['Mean_h6'] = group['Log_Return_h6'].rolling(window=330).mean().shift(6)\n",
        "    group['Mean_h12'] = group['Log_Return_h12'].rolling(window=330).mean().shift(12)\n",
        "    # Append the results to df\n",
        "    mean_dfs.append(group)\n",
        "\n",
        "actual_data = pd.concat(mean_dfs).reset_index()\n",
        "MeanBenchmark = actual_data[['Date', 'GEO_Name', 'Mean_h1', 'Mean_h3', 'Mean_h6', 'Mean_h12']].rename(columns={'GEO_Name': 'state', 'Mean_h1': 'h1', 'Mean_h3': 'h3', 'Mean_h6': 'h6', 'Mean_h12': 'h12'})"
      ],
      "metadata": {
        "id": "PVeyZ2Hd6Tqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "qXCaTN8A6UVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading predictions and corresponding actuals\n",
        "actuals = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(1)Actuals.csv\", sep=',')\n",
        "AR_1 = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(1)Predictions.csv\", sep=',')\n",
        "AR_optimal = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/AR(Optimal)Predictions.csv\", sep=',')\n",
        "ARIMA = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/ARIMAPredictions.csv\", sep=',')\n",
        "RandomForest = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/RandomForestPredictions.csv\", sep=',')\n",
        "XGBoost = pd.read_csv(\"/content/drive/MyDrive/Thesis/Models/Predictions/XGBoostPredictions.csv\", sep=',')"
      ],
      "metadata": {
        "id": "fFDJyErE6Tvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the date column to date\n",
        "actuals.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "AR_1.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "AR_optimal.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "ARIMA.rename(columns={'PredictionDate': 'Date'}, inplace=True)\n",
        "\n",
        "RandomForest.rename(columns={'OriginalIndex': 'Date'}, inplace=True)\n",
        "XGBoost.rename(columns={'OriginalIndex': 'Date'}, inplace=True)\n",
        "\n",
        "RandomForest.rename(columns={'State': 'state'}, inplace=True)\n",
        "XGBoost.rename(columns={'State': 'state'}, inplace=True)"
      ],
      "metadata": {
        "id": "QKZAZu0K6Ylc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Combination forecasts\n",
        "combined_df1 = pd.concat([AR_optimal, ARIMA, RandomForest, XGBoost], ignore_index=True)\n",
        "averaged_predictions1 = combined_df1.groupby(['Date', 'state']).mean().reset_index()\n",
        "Combination_AR_1 = averaged_predictions1.sort_values(by=['state', 'Date']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "T_sDSN19vCZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing 'USA' from the predictions and actuals\n",
        "actuals = actuals[actuals['state'] != 'USA']\n",
        "AR_1 = AR_1[AR_1['state'] != 'USA']\n",
        "AR_optimal = AR_optimal[AR_optimal['state'] != 'USA']\n",
        "ARIMA = ARIMA[ARIMA['state'] != 'USA']\n",
        "RandomForest = RandomForest[RandomForest['state'] != 'USA']\n",
        "XGBoost = XGBoost[XGBoost['state'] != 'USA']\n",
        "Combination_AR_1 = Combination_AR_1[Combination_AR_1['state'] != 'USA']\n",
        "MeanBenchmark = MeanBenchmark[MeanBenchmark['state'] != 'USA']"
      ],
      "metadata": {
        "id": "Fra8a1U9YGr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the dataframes to datetime\n",
        "actuals['Date'] = pd.to_datetime(actuals['Date'])\n",
        "AR_1['Date'] = pd.to_datetime(AR_1['Date'])\n",
        "AR_optimal['Date'] = pd.to_datetime(AR_optimal['Date'])\n",
        "ARIMA['Date'] = pd.to_datetime(ARIMA['Date'])\n",
        "RandomForest['Date'] = pd.to_datetime(RandomForest['Date'])\n",
        "XGBoost['Date'] = pd.to_datetime(XGBoost['Date'])\n",
        "Combination_AR_1['Date'] = pd.to_datetime(Combination_AR_1['Date'])\n",
        "MeanBenchmark['Date'] = pd.to_datetime(MeanBenchmark['Date'])"
      ],
      "metadata": {
        "id": "2lQ0iUxw6Y3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the dataframes to have the same length\n",
        "actuals = actuals[actuals['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "AR_1 = AR_1[AR_1['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "AR_optimal = AR_optimal[AR_optimal['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "ARIMA = ARIMA[ARIMA['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "RandomForest = RandomForest[RandomForest['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "XGBoost = XGBoost[XGBoost['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "Combination_AR_1 = Combination_AR_1[Combination_AR_1['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)\n",
        "MeanBenchmark = MeanBenchmark[MeanBenchmark['Date'] >= pd.Timestamp('2008-07-01')].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "2LXQPv7MmIK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MSE and MAE across states"
      ],
      "metadata": {
        "id": "QrsVbkq89-w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE"
      ],
      "metadata": {
        "id": "wHU4E3Jo-4KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models used in making MSE over time\n",
        "models = {\n",
        "    'Benchmark': MeanBenchmark,\n",
        "    'AR_1': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination_AR_1': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "yHWErxsz-E5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_by_state = {}\n",
        "\n",
        "states = actuals['state'].unique()\n",
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "\n",
        "for state in states:\n",
        "    state_mse = {}\n",
        "    for model_name, df_model in models.items():\n",
        "        mse_per_horizon = {}\n",
        "        for horizon in horizons:\n",
        "            true_values = actuals[actuals['state'] == state][horizon]\n",
        "            predictions = df_model[df_model['state'] == state][horizon]\n",
        "\n",
        "            # Calculating MSE\n",
        "            mse_per_horizon[horizon] = mean_squared_error(true_values, predictions)\n",
        "\n",
        "        # Appending to df's\n",
        "        state_mse[model_name] = mse_per_horizon\n",
        "    mse_by_state[state] = state_mse"
      ],
      "metadata": {
        "id": "BtQGhz_I-Nmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating df's for MSE\n",
        "rows = []\n",
        "for state, models in mse_by_state.items():\n",
        "    for model, horizons in models.items():\n",
        "        for horizon, MSE in horizons.items():\n",
        "            rows.append({\n",
        "                'State': state,\n",
        "                'Model': model,\n",
        "                'Horizon': horizon,\n",
        "                'MSE': MSE\n",
        "            })\n",
        "\n",
        "mse_state = pd.DataFrame(rows)\n",
        "mse_state.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "9OIEfOdG-PwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_state.to_csv(\"/content/drive/MyDrive/Thesis/Performance Calculations/MSE_States.csv\")\n",
        "mse_state.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/MSE_States.xlsx\")"
      ],
      "metadata": {
        "id": "-Wgs0gA3-Skr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAE"
      ],
      "metadata": {
        "id": "_4APbrLO-YUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models used in making MSE over time\n",
        "models = {\n",
        "    'Benchmark': MeanBenchmark,\n",
        "    'AR_1': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination_AR_1': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "SzpSSUEd-ZyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_by_state = {}\n",
        "states = actuals['state'].unique()\n",
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "\n",
        "for state in states:\n",
        "    state_mae = {}\n",
        "    for model_name, df_model in models.items():\n",
        "        mae_per_horizon = {}\n",
        "        for horizon in horizons:\n",
        "            true_values = actuals[actuals['state'] == state][horizon]\n",
        "            predictions = df_model[df_model['state'] == state][horizon]\n",
        "\n",
        "            # Calculating MAE\n",
        "            mae_per_horizon[horizon] = mean_absolute_error(true_values, predictions)\n",
        "\n",
        "        state_mae[model_name] = mae_per_horizon\n",
        "    mae_by_state[state] = state_mae"
      ],
      "metadata": {
        "id": "SGIIBFXE-el3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating df's for MAE\n",
        "rows = []\n",
        "for state, models in mae_by_state.items():\n",
        "    for model, horizons in models.items():\n",
        "        for horizon, MAE in horizons.items():\n",
        "            rows.append({\n",
        "                'State': state,\n",
        "                'Model': model,\n",
        "                'Horizon': horizon,\n",
        "                'MAE': MAE\n",
        "            })\n",
        "\n",
        "mae_state = pd.DataFrame(rows)\n",
        "mae_state.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "wKTRFYKb-gUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_state.to_csv(\"/content/drive/MyDrive/Thesis/Performance Calculations/MAE_States.csv\")\n",
        "mae_state.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/MAE_States.xlsx\")"
      ],
      "metadata": {
        "id": "IJyH0HnJ-o5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ratio model comparisons (MEAN)"
      ],
      "metadata": {
        "id": "Am7nv4ET6hYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE Ratios"
      ],
      "metadata": {
        "id": "HPQL-LJYW0Vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of model dataframes and their identifiers\n",
        "models = {\n",
        "    'Benchmark': MeanBenchmark,\n",
        "    'AR_1': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "RUFkPgn86bun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_by_state = {}\n",
        "\n",
        "states = actuals['state'].unique()\n",
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "\n",
        "for state in states:\n",
        "    state_mse = {}\n",
        "    for model_name, df_model in models.items():\n",
        "        mse_per_horizon = {}\n",
        "        for horizon in horizons:\n",
        "            true_values = actuals[actuals['state'] == state][horizon]\n",
        "            predictions = df_model[df_model['state'] == state][horizon]\n",
        "\n",
        "            # Calculating MSE\n",
        "            mse_per_horizon[horizon] = mean_squared_error(true_values, predictions)\n",
        "\n",
        "        # Appending to df's\n",
        "        state_mse[model_name] = mse_per_horizon\n",
        "    mse_by_state[state] = state_mse"
      ],
      "metadata": {
        "id": "zerKL-QGQFGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Ratios compared to benchmark\n",
        "mse_ratios_by_state = {}\n",
        "\n",
        "for state, mse_data in mse_by_state.items():\n",
        "    state_ratios = {}\n",
        "    benchmark_mse = mse_data['Benchmark']\n",
        "    for model, mses in mse_data.items():\n",
        "        ratios = {horizon: mse / benchmark_mse[horizon] if benchmark_mse[horizon] != 0 else None for horizon, mse in mses.items()}\n",
        "        state_ratios[model] = ratios\n",
        "    mse_ratios_by_state[state] = state_ratios"
      ],
      "metadata": {
        "id": "RIhOCg3PRIL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appending data to df's skipping the benchmark\n",
        "data = []\n",
        "for state, models in mse_ratios_by_state.items():\n",
        "    for model, ratios in models.items():\n",
        "        if model != 'Benchmark':\n",
        "            for horizon, ratio in ratios.items():\n",
        "                data.append((state, model, horizon, ratio))\n",
        "\n",
        "df_ratios = pd.DataFrame(data, columns=['State', 'Model', 'Horizon', 'MSE Ratio']).sort_values(by=['State', 'Horizon'])\n",
        "df_ratios.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "k7duTMDyRK4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratios.to_csv(\"/content/drive/MyDrive/Thesis/Performance Calculations/MSERatiosAllHorizonsMEANBenchmark.csv\")\n",
        "df_ratios.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/MSERatiosAllHorizonsMEANBenchmark.xlsx\")"
      ],
      "metadata": {
        "id": "HU0-SB-6RNIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAE Ratios"
      ],
      "metadata": {
        "id": "VS6P4NlnW3S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of model dataframes and their identifiers\n",
        "models = {\n",
        "    'Benchmark': MeanBenchmark,\n",
        "    'AR_1': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "dxph4sydY89s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_by_state = {}\n",
        "states = actuals['state'].unique()\n",
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "\n",
        "for state in states:\n",
        "    state_mae = {}\n",
        "    for model_name, df_model in models.items():\n",
        "        mae_per_horizon = {}\n",
        "        for horizon in horizons:\n",
        "            true_values = actuals[actuals['state'] == state][horizon]\n",
        "            predictions = df_model[df_model['state'] == state][horizon]\n",
        "\n",
        "            # Calculating MAE\n",
        "            mae_per_horizon[horizon] = mean_absolute_error(true_values, predictions)\n",
        "\n",
        "        state_mae[model_name] = mae_per_horizon\n",
        "    mae_by_state[state] = state_mae"
      ],
      "metadata": {
        "id": "cMyTxVYRW6cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Ratios compared to benchmark\n",
        "mae_ratios_by_state = {}\n",
        "\n",
        "for state, mae_data in mae_by_state.items():\n",
        "    state_ratios = {}\n",
        "    benchmark_mae = mae_data['Benchmark']\n",
        "    for model, maes in mae_data.items():\n",
        "        ratios = {horizon: mae / benchmark_mae[horizon] if benchmark_mae[horizon] != 0 else None for horizon, mae in maes.items()}\n",
        "        state_ratios[model] = ratios\n",
        "    mae_ratios_by_state[state] = state_ratios"
      ],
      "metadata": {
        "id": "X8bVRwrdXF5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appending data to df's skipping the benchmark\n",
        "data = []\n",
        "for state, models in mae_ratios_by_state.items():\n",
        "    for model, ratios in models.items():\n",
        "        if model != 'Benchmark':\n",
        "            for horizon, ratio in ratios.items():\n",
        "                data.append((state, model, horizon, ratio))\n",
        "\n",
        "df_ratios = pd.DataFrame(data, columns=['State', 'Model', 'Horizon', 'mae Ratio']).sort_values(by=['State', 'Horizon'])\n",
        "df_ratios.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "EKtKBMruXJA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratios.to_csv(\"/content/drive/MyDrive/Thesis/Performance Calculations/MAERatiosAllHorizonsMEANBenchmark.csv\")\n",
        "df_ratios.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/MAERatiosAllHorizonsMEANBenchmark.xlsx\")"
      ],
      "metadata": {
        "id": "MbVaA1E6XQCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ratio model comparisons (AR_1)"
      ],
      "metadata": {
        "id": "aU8aBOah8AiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MSE Ratios"
      ],
      "metadata": {
        "id": "LGxdLTLfXxUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of model dataframes and their identifiers\n",
        "models = {\n",
        "    'Benchmark': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "VMpzHVRhQjWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_by_state = {}\n",
        "\n",
        "states = actuals['state'].unique()\n",
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "\n",
        "for state in states:\n",
        "    state_mse = {}\n",
        "    for model_name, df_model in models.items():\n",
        "        mse_per_horizon = {}\n",
        "        for horizon in horizons:\n",
        "            true_values = actuals[actuals['state'] == state][horizon]\n",
        "            predictions = df_model[df_model['state'] == state][horizon]\n",
        "\n",
        "            # Calculating MSE\n",
        "            mse_per_horizon[horizon] = mean_squared_error(true_values, predictions)\n",
        "\n",
        "        # Appending to df's\n",
        "        state_mse[model_name] = mse_per_horizon\n",
        "    mse_by_state[state] = state_mse"
      ],
      "metadata": {
        "id": "2ms8d2x7Xr6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Ratios compared to benchmark\n",
        "mse_ratios_by_state = {}\n",
        "\n",
        "for state, mse_data in mse_by_state.items():\n",
        "    state_ratios = {}\n",
        "    benchmark_mse = mse_data['Benchmark']\n",
        "    for model, mses in mse_data.items():\n",
        "        ratios = {horizon: mse / benchmark_mse[horizon] if benchmark_mse[horizon] != 0 else None for horizon, mse in mses.items()}\n",
        "        state_ratios[model] = ratios\n",
        "    mse_ratios_by_state[state] = state_ratios"
      ],
      "metadata": {
        "id": "dcrpHc_K8CrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appending data to df's skipping the benchmark\n",
        "data = []\n",
        "for state, models in mse_ratios_by_state.items():\n",
        "    for model, ratios in models.items():\n",
        "        if model != 'Benchmark':\n",
        "            for horizon, ratio in ratios.items():\n",
        "                data.append((state, model, horizon, ratio))\n",
        "\n",
        "df_ratios = pd.DataFrame(data, columns=['State', 'Model', 'Horizon', 'MSE Ratio']).sort_values(by=['State', 'Horizon'])\n",
        "df_ratios.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "QylyeScQ9ifl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratios.to_csv(\"/content/drive/MyDrive/Thesis/Performance Calculations/MSERatiosAllHorizonsAR(1)Benchmark.csv\")\n",
        "df_ratios.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/MSERatiosAllHorizonsAR(1)Benchmark.xlsx\")"
      ],
      "metadata": {
        "id": "xUStxK1h_DFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAE Ratios"
      ],
      "metadata": {
        "id": "GNJczf03X_LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of model dataframes and their identifiers\n",
        "models = {\n",
        "    'Benchmark': AR_1,\n",
        "    'XGBoost': XGBoost,\n",
        "    'RandomForest': RandomForest,\n",
        "    'AR_optimal': AR_optimal,\n",
        "    'ARIMA': ARIMA,\n",
        "    'Combination': Combination_AR_1\n",
        "}"
      ],
      "metadata": {
        "id": "HM95eciuZD6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae_by_state = {}\n",
        "states = actuals['state'].unique()\n",
        "horizons = ['h1', 'h3', 'h6', 'h12']\n",
        "\n",
        "for state in states:\n",
        "    state_mae = {}\n",
        "    for model_name, df_model in models.items():\n",
        "        mae_per_horizon = {}\n",
        "        for horizon in horizons:\n",
        "            true_values = actuals[actuals['state'] == state][horizon]\n",
        "            predictions = df_model[df_model['state'] == state][horizon]\n",
        "\n",
        "            # Calculating MAE\n",
        "            mae_per_horizon[horizon] = mean_absolute_error(true_values, predictions)\n",
        "\n",
        "        state_mae[model_name] = mae_per_horizon\n",
        "    mae_by_state[state] = state_mae"
      ],
      "metadata": {
        "id": "VMCz4w_4YAMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Ratios compared to benchmark\n",
        "mae_ratios_by_state = {}\n",
        "\n",
        "for state, mae_data in mae_by_state.items():\n",
        "    state_ratios = {}\n",
        "    benchmark_mae = mae_data['Benchmark']\n",
        "    for model, maes in mae_data.items():\n",
        "        ratios = {horizon: mae / benchmark_mae[horizon] if benchmark_mae[horizon] != 0 else None for horizon, mae in maes.items()}\n",
        "        state_ratios[model] = ratios\n",
        "    mae_ratios_by_state[state] = state_ratios"
      ],
      "metadata": {
        "id": "s-0vSrH1YAoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Appending data to df's skipping the benchmark\n",
        "data = []\n",
        "for state, models in mae_ratios_by_state.items():\n",
        "    for model, ratios in models.items():\n",
        "        if model != 'Benchmark':\n",
        "            for horizon, ratio in ratios.items():\n",
        "                data.append((state, model, horizon, ratio))\n",
        "\n",
        "df_ratios = pd.DataFrame(data, columns=['State', 'Model', 'Horizon', 'mae Ratio']).sort_values(by=['State', 'Horizon'])\n",
        "df_ratios.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "JF6sVL8eYAqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ratios.to_csv(\"/content/drive/MyDrive/Thesis/Performance Calculations/MAERatiosAllHorizonsAR(1)Benchmark.csv\")\n",
        "df_ratios.to_excel(\"/content/drive/MyDrive/Thesis/Performance Calculations/MAERatiosAllHorizonsAR(1)Benchmark.xlsx\")"
      ],
      "metadata": {
        "id": "6VaODsliYAst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "5A1wyhX1Vo4u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}