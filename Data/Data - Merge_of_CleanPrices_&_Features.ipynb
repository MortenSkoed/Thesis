{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "pkbzsm47FjmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RkW4nJ5A55f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "N7MXaCIXFmSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "path1 = \"/content/drive/MyDrive/Thesis/Data/Data/Cleaned_Price.csv\"\n",
        "price = pd.read_csv(path1, sep=',')\n",
        "\n",
        "path2 = \"/content/drive/MyDrive/Thesis/Data/Data/FRED MD - National level explanatory data.csv\"\n",
        "variables = pd.read_csv(path2, sep=';')"
      ],
      "metadata": {
        "id": "r3qYauS3BFRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ],
      "metadata": {
        "id": "2_QKDxTpF1dO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprossing data\n",
        "price['Date'] = pd.to_datetime(price['Date'])\n",
        "variables['Date'] = pd.to_datetime(variables['Date'])\n",
        "variables.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "id": "yoZTOaeoF3RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datetime_index_col = 'Date'\n",
        "\n",
        "variables['HWI'] = variables['HWI'].astype(float)\n",
        "\n",
        "for x in variables.columns:\n",
        "    if variables[x].dtype == 'object':\n",
        "        variables[x] = variables[x].str.replace(',' , '.')\n",
        "        variables[x] = pd.to_numeric(variables[x], errors='coerce')"
      ],
      "metadata": {
        "id": "wCjQzKnpBHX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the start date of data\n",
        "specific_date = '1975-01-01'\n",
        "specific_date = pd.Timestamp(specific_date)\n",
        "variables = variables.loc[specific_date:]"
      ],
      "metadata": {
        "id": "CEsuL9n8BJtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting missing values from FRED-MD\n",
        "non_null_counts = variables.count()\n",
        "columns_with_missing_values = non_null_counts[non_null_counts != 588]\n",
        "\n",
        "print(\"Columns with missing values:\")\n",
        "for column, count in columns_with_missing_values.items():\n",
        "    print(f\"{column}: {588 - count} missing values\")\n",
        "\n",
        "print(\"Total number of columns with missing values:\", len(columns_with_missing_values))"
      ],
      "metadata": {
        "id": "8TpwhrG5BMB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping values that is recorded later than the dataset start\n",
        "drop = ['ACOGNO', 'UMCSENTx']\n",
        "variables.drop(columns=drop, inplace=True)"
      ],
      "metadata": {
        "id": "gEVmioNfBN_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if the dropped variables are actually dropped\n",
        "non_null_counts = variables.count()\n",
        "columns_with_missing_values = non_null_counts[non_null_counts != 588]\n",
        "\n",
        "print(\"Columns with missing values:\")\n",
        "for column, count in columns_with_missing_values.items():\n",
        "    print(f\"{column}: {588 - count} missing values\")\n",
        "\n",
        "print(\"Total number of columns with missing values:\", len(columns_with_missing_values))"
      ],
      "metadata": {
        "id": "NmqANOzjBeDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward imputating in variables that has max 6 missing values\n",
        "variables = variables.ffill(axis=0)\n",
        "selected_columns = variables[['CP3Mx', 'COMPAPFFx', 'S&P div yield', 'S&P PE ratio']]\n",
        "selected_columns"
      ],
      "metadata": {
        "id": "TkusIDK5BWbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the forward imputation mechanism\n",
        "\n",
        "non_null_counts = variables.count()\n",
        "columns_with_missing_values = non_null_counts[non_null_counts != 588]\n",
        "\n",
        "print(\"Columns with missing values:\")\n",
        "for column, count in columns_with_missing_values.items():\n",
        "    print(f\"{column}: {588 - count} missing values\")\n",
        "\n",
        "print(\"Total number of columns with missing values:\", len(columns_with_missing_values))"
      ],
      "metadata": {
        "id": "6xhE4874DaZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concat FRED-MD data on each state\n",
        "repeat = pd.concat([variables] * 51, ignore_index=True)\n",
        "result = pd.concat([price, repeat], axis=1)"
      ],
      "metadata": {
        "id": "f_MB3wmuBkUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regions Data from FRED-MD"
      ],
      "metadata": {
        "id": "GVwxLyxTHBPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "path3 = \"/content/drive/MyDrive/Thesis/Data/Data/State Regions.csv\"\n",
        "regions = pd.read_csv(path3, sep=';')"
      ],
      "metadata": {
        "id": "RGFichhUE7TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the region configuration on each state\n",
        "merged_df = pd.merge(result, regions[['State', 'Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)']], left_on='GEO_Name', right_on='State', how='left')\n",
        "merged_df.drop('State', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "9-wHsbF6LVA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making HOUST into 1 column based on their region"
      ],
      "metadata": {
        "id": "TlQhnzKGPrWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for 'HOUST'\n",
        "merged_df['houst'] = None"
      ],
      "metadata": {
        "id": "UvBKLwWdQCGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HOUST_column = merged_df.pop('HOUST')\n",
        "merged_df['HOUST'] = HOUST_column\n",
        "HOUSTNE_column = merged_df.pop('HOUSTNE')\n",
        "merged_df['HOUSTNE'] = HOUSTNE_column\n",
        "HOUSTMW_column = merged_df.pop('HOUSTMW')\n",
        "merged_df['HOUSTMW'] = HOUSTMW_column\n",
        "HOUSTS_column = merged_df.pop('HOUSTS')\n",
        "merged_df['HOUSTS'] = HOUSTS_column\n",
        "HOUSTW_column = merged_df.pop('HOUSTW')\n",
        "merged_df['HOUSTW'] = HOUSTW_column"
      ],
      "metadata": {
        "id": "NUgAg9sJOCfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 1, 'houst'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 1, 'HOUSTNE']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 2, 'houst'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 2, 'HOUSTMW']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 3, 'houst'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 3, 'HOUSTS']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 4, 'houst'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 4, 'HOUSTW']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 5, 'houst'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 5, 'HOUST']"
      ],
      "metadata": {
        "id": "NjOuR2KKNqNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping excess columns\n",
        "columns_to_drop = ['HOUST', 'HOUSTNE', 'HOUSTMW', 'HOUSTS', 'HOUSTW']\n",
        "merged_df.drop(columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "fmPLHZJnPf-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the variable\n",
        "merged_df.rename(columns={'houst': 'HOUST'}, inplace=True)"
      ],
      "metadata": {
        "id": "-zqYZkzIPomx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making PERMIT into 1 column based on their region"
      ],
      "metadata": {
        "id": "3N7REJXePvxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for 'PERMIT'\n",
        "merged_df['permit'] = None"
      ],
      "metadata": {
        "id": "GQihR7LLP_Le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PERMIT_column = merged_df.pop('PERMIT')\n",
        "merged_df['PERMIT'] = PERMIT_column\n",
        "PERMITNE_column = merged_df.pop('PERMITNE')\n",
        "merged_df['PERMITNE'] = PERMITNE_column\n",
        "PERMITMW_column = merged_df.pop('PERMITMW')\n",
        "merged_df['PERMITMW'] = PERMITMW_column\n",
        "PERMITS_column = merged_df.pop('PERMITS')\n",
        "merged_df['PERMITS'] = PERMITS_column\n",
        "PERMITW_column = merged_df.pop('PERMITW')\n",
        "merged_df['PERMITW'] = PERMITW_column"
      ],
      "metadata": {
        "id": "VqC5hEmIPwuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 1, 'permit'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 1, 'PERMITNE']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 2, 'permit'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 2, 'PERMITMW']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 3, 'permit'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 3, 'PERMITS']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 4, 'permit'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 4, 'PERMITW']\n",
        "merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 5, 'permit'] = merged_df.loc[merged_df['Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'] == 5, 'PERMIT']"
      ],
      "metadata": {
        "id": "2NdcdaVvQHsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping excess variables\n",
        "columns_to_drop = ['PERMIT', 'PERMITNE', 'PERMITMW', 'PERMITS', 'PERMITW']\n",
        "merged_df.drop(columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XBMuSeLfRBuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the variable\n",
        "merged_df.rename(columns={'permit': 'PERMIT'}, inplace=True)"
      ],
      "metadata": {
        "id": "PSCwDUIPRFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and transformation of Sentiment variables"
      ],
      "metadata": {
        "id": "UZ15N3F_9wDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "path6 = \"/content/drive/MyDrive/Thesis/Data/Data/Michigan Survey - Sentiment - All.csv\"\n",
        "sentiment = pd.read_csv(path6, sep=';')\n",
        "\n",
        "sentiment['Date'] = pd.to_datetime(sentiment['Date'])"
      ],
      "metadata": {
        "id": "XAdIOdXC93L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge dataframes from FRED-MD and Michigan Survey\n",
        "merged_data = pd.merge(merged_df, sentiment, on=['Date', 'Region (1=NE, 2=MW, 3=S, 4=W, 5=USA)'], how='left')\n",
        "merged_data.dropna(inplace=True)\n",
        "ALL_DATA = merged_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "VxlrGjpdKFF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping 'GEO_Types' and 'Index_NSA'\n",
        "columns = ['GEO_Type', 'Index_NSA']\n",
        "ALL_DATA.drop(columns=columns, inplace=True)\n",
        "final = ALL_DATA.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "a5j4Sw2XFx6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taking Ln of Index_SA"
      ],
      "metadata": {
        "id": "pdIrOEILVvBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final['Ln(Index_SA)'] = np.log(final['Index_SA'])"
      ],
      "metadata": {
        "id": "7dO2e7aHV9m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Log Returns at different horizons\n"
      ],
      "metadata": {
        "id": "T3-WF_sbXPXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining all unique states\n",
        "state_abbreviations = final['GEO_Name'].unique()\n",
        "\n",
        "horizons = [1, 3, 6, 12]\n",
        "\n",
        "def calculate_log_returns(df, horizons):\n",
        "    for h in horizons:\n",
        "        df[f'Log_Return_h{h}'] = df['Ln(Index_SA)'] - df['Ln(Index_SA)'].shift(h)\n",
        "    return df\n",
        "\n",
        "# Applying the function to each state separately and concat the results\n",
        "final_with_returns = pd.concat(\n",
        "    [calculate_log_returns(final[final['GEO_Name'] == state_abbr].copy(), horizons) for state_abbr in state_abbreviations]\n",
        ")\n",
        "\n",
        "final_with_returns.reset_index(drop=True, inplace=True)\n",
        "final_with_returns.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "id": "GXuWho7sXTW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the merged data frame as CSV"
      ],
      "metadata": {
        "id": "A2F5_AbMt4wN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_with_returns.to_csv(\"/content/drive/MyDrive/Thesis/Data/Merging CleanPrice & Features.csv\")"
      ],
      "metadata": {
        "id": "2Yq7xqeaFEfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "_knTC6p3VqsU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}